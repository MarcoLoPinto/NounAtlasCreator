{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fe2040e",
   "metadata": {
    "papermill": {
     "duration": 0.078963,
     "end_time": "2022-03-31T22:14:09.386664",
     "exception": false,
     "start_time": "2022-03-31T22:14:09.307701",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marco/miniconda3/envs/nuans_minihw2/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5424a631",
   "metadata": {
    "papermill": {
     "duration": 0.049244,
     "end_time": "2022-03-31T22:14:12.552634",
     "exception": false,
     "start_time": "2022-03-31T22:14:12.503390",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Important paths for the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_root_path = './datasets/'\n",
    "srl_dataset_path = os.path.join(datasets_root_path, 'united-srl')\n",
    "checkpoints_dir_path = './checkpoints/'\n",
    "model_dir_path = os.path.join(checkpoints_dir_path, 'model_pid')\n",
    "\n",
    "srl_dataset_dict_paths = {}\n",
    "for lang in os.listdir(srl_dataset_path):\n",
    "    dataset_lang_path = os.path.join(srl_dataset_path, lang)\n",
    "    if os.path.isdir(dataset_lang_path):\n",
    "        srl_dataset_dict_paths[lang] = {}\n",
    "        for d_type in os.listdir(dataset_lang_path):\n",
    "            d_name = d_type.split('.')[0]\n",
    "            srl_dataset_dict_paths[lang][d_name] = os.path.join(dataset_lang_path, d_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bc95ee",
   "metadata": {
    "papermill": {
     "duration": 0.048851,
     "end_time": "2022-03-31T22:14:13.015246",
     "exception": false,
     "start_time": "2022-03-31T22:14:12.966395",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Setting the seed for reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 28\n",
    "\n",
    "# random.seed(SEED) # not used\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_params = {\n",
    "    'batch_size': 32,\n",
    "    'transformer_name': \"xlm-roberta-base\",\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_files.datasets.srl_transformer import SRLDataset_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_en = SRLDataset_transformer(  srl_dataset_dict_paths['EN']['train'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_params.update({\n",
    "    'n_roles_labels': len(dataset_train_en.id_to_roles),\n",
    "    'n_predicates_labels': len(dataset_train_en.id_to_predicates),\n",
    "\n",
    "    'id_to_roles': dataset_train_en.id_to_roles,\n",
    "    'roles_to_id': dataset_train_en.roles_to_id,\n",
    "    'roles_pad_id': dataset_train_en.roles_pad_id,\n",
    "    'roles_pad': dataset_train_en.roles_pad,\n",
    "\n",
    "    'id_to_predicates': dataset_train_en.id_to_predicates,\n",
    "    'predicates_to_id': dataset_train_en.predicates_to_id,\n",
    "    'predicates_pad_id': dataset_train_en.predicates_pad_id,\n",
    "    'predicates_pad': dataset_train_en.predicates_pad,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(model_dir_path, 'global_params.npy'), global_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5501"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_train_en.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dependency_heads': [3, 3, 0, 8, 7, 7, 8, 3, 10, 8, 13, 13, 10, 3],\n",
       " 'dependency_relations': ['nsubj',\n",
       "  'advmod',\n",
       "  'root',\n",
       "  'mark',\n",
       "  'det',\n",
       "  'amod',\n",
       "  'nsubj',\n",
       "  'ccomp',\n",
       "  'amod',\n",
       "  'obj',\n",
       "  'case',\n",
       "  'amod',\n",
       "  'nmod',\n",
       "  'punct'],\n",
       " 'lemmas': ['member',\n",
       "  'also',\n",
       "  'ask',\n",
       "  'whether',\n",
       "  'all',\n",
       "  'social',\n",
       "  'group',\n",
       "  'enjoy',\n",
       "  'equal',\n",
       "  'access',\n",
       "  'to',\n",
       "  'higher',\n",
       "  'education',\n",
       "  '.'],\n",
       " 'pos_tags': ['NOUN',\n",
       "  'ADV',\n",
       "  'VERB',\n",
       "  'SCONJ',\n",
       "  'DET',\n",
       "  'ADJ',\n",
       "  'NOUN',\n",
       "  'VERB',\n",
       "  'ADJ',\n",
       "  'NOUN',\n",
       "  'ADP',\n",
       "  'ADJ',\n",
       "  'NOUN',\n",
       "  'PUNCT'],\n",
       " 'predicates': ['_',\n",
       "  '_',\n",
       "  'ASK_REQUEST',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  'BENEFIT_EXPLOIT',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_'],\n",
       " 'roles': {'2': ['agent',\n",
       "   '_',\n",
       "   '_',\n",
       "   'theme',\n",
       "   '_',\n",
       "   '_',\n",
       "   '_',\n",
       "   '_',\n",
       "   '_',\n",
       "   '_',\n",
       "   '_',\n",
       "   '_',\n",
       "   '_',\n",
       "   '_'],\n",
       "  '7': ['_',\n",
       "   '_',\n",
       "   '_',\n",
       "   '_',\n",
       "   '_',\n",
       "   '_',\n",
       "   'beneficiary',\n",
       "   '_',\n",
       "   '_',\n",
       "   'theme',\n",
       "   '_',\n",
       "   '_',\n",
       "   '_',\n",
       "   '_']},\n",
       " 'words': ['Members',\n",
       "  'also',\n",
       "  'asked',\n",
       "  'whether',\n",
       "  'all',\n",
       "  'social',\n",
       "  'groups',\n",
       "  'enjoyed',\n",
       "  'equal',\n",
       "  'access',\n",
       "  'to',\n",
       "  'higher',\n",
       "  'education',\n",
       "  '.']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train_en[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader_train_en = DataLoader(\n",
    "    dataset_train_en,\n",
    "    batch_size=global_params['batch_size'],\n",
    "    collate_fn=dataset_train_en.create_collate_fn(),\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in dataloader_train_en:\n",
    "    ex_in = e\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'view', 'was', 'expressed', 'that', ',', 'to', 'avoid', 'duplication', 'and', 'to', 'identify', 'common', 'practices', ',', 'a', 'proper', 'division', 'of', 'labour', 'was', 'required', 'between', 'the', 'Department', 'of', 'Economic', 'and', 'Social', 'Affairs', 'and', 'the', 'regional', 'commissions', ',', 'as', 'well', 'as', 'among', 'the', 'regional', 'commissions', '.'] \n",
      " ['_', '_', '_', 'SHOW', '_', '_', '_', 'ABSTAIN_AVOID_REFRAIN', '_', '_', '_', 'RECOGNIZE_ADMIT_IDENTIFY', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'REQUIRE_NEED_WANT_HOPE', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'] \n",
      " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(ex_in['words'][1], '\\n', ex_in['predicates'][1], '\\n', ex_in['predicates_positions'][1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the PID model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss(ignore_index = dataset_train_en.predicates_pad_id) # !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from code_files.models.model_pid import ModelPID\n",
    "from code_files.utils.utils_functions import print_summary\n",
    "import torch.optim as optim\n",
    "\n",
    "model_pid = ModelPID(\n",
    "    loss_fn = loss_function,\n",
    "    hparams = global_params,\n",
    "    fine_tune_transformer = True,\n",
    "    has_predicates_positions = False # it does both pred iden and disamb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model_pid.parameters(), lr=0.0016, momentum=0.9) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_files.utils.Trainer_pid import Trainer_pid\n",
    "\n",
    "trainer_pid = Trainer_pid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 87]) torch.Size([32, 87, 433])\n"
     ]
    }
   ],
   "source": [
    "d = trainer_pid.compute_forward(model_pid, ex_in, device, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, batch_encoding = model_pid.forward(ex_in['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_', '_', '_', 'SHOW', '_', '_', '_', 'ABSTAIN_AVOID_REFRAIN', '_', '_', '_', 'RECOGNIZE_ADMIT_IDENTIFY', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'REQUIRE_NEED_WANT_HOPE', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_']\n",
      "[None, 0, 1, 2, 3, 3, 4, 5, 5, 6, 7, 8, 8, 9, 10, 11, 12, 13, 13, 14, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 33, 34, 34, 35, 36, 37, 38, 39, 40, 41, 41, 42, 42, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "print(ex_in['predicates'][1])\n",
    "print(batch_encoding.word_ids(batch_index=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -1,   0,   0,   0, 364,  -1,   0,   0,  -1,   0,   2,   0,  -1,   0,\n",
      "           0, 318,   0,   0,  -1,   0,  -1,   0,   0,   0,   0,   0,   0, 335,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  -1,   0,\n",
      "          -1,   0,   0,   0,   0,   0,   0,   0,  -1,   0,  -1,  -1,  -1,  -1,\n",
      "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
      "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
      "          -1,  -1,  -1],\n",
      "        [ -1,   0,   0,   0, 364,  -1,   0,   0,  -1,   0,   2,   0,  -1,   0,\n",
      "           0, 318,   0,   0,  -1,   0,  -1,   0,   0,   0,   0,   0,   0, 335,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  -1,   0,\n",
      "          -1,   0,   0,   0,   0,   0,   0,   0,  -1,   0,  -1,  -1,  -1,  -1,\n",
      "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
      "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
      "          -1,  -1,  -1]])\n"
     ]
    }
   ],
   "source": [
    "words_ids = batch_encoding.word_ids(batch_index = 1)\n",
    "label_processed = [\n",
    "    model_pid.hparams['predicates_pad_id'] if v == None or (v != None and j-1>=0 and words_ids[j-1]==words_ids[j])\n",
    "    else model_pid.hparams['predicates_to_id'][ ex_in['predicates'][1][v] ]\n",
    "    for j,v in enumerate(words_ids)\n",
    "]\n",
    "print(torch.stack([torch.tensor(label_processed), torch.tensor(label_processed)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1205472])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0039,  0.2021,  0.6279,  0.6723,  0.5644,  0.6415,  0.1043,  0.6775,\n",
       "         0.5614,  0.4894,  0.5786,  0.6831,  0.5759,  0.7422,  0.6217,  0.0986,\n",
       "         0.7288,  0.7039,  0.7125,  0.6595,  0.0470,  0.6807,  0.6311,  0.6803,\n",
       "         0.6754,  0.0680,  0.6922,  0.5441,  0.1592,  0.7077,  0.6492,  0.7187,\n",
       "         0.6400,  0.7136,  0.7484,  0.1834,  0.1921,  0.1394,  0.7266,  0.2045,\n",
       "         0.8026,  0.6935,  0.5865,  0.6511,  0.6731,  0.5897,  0.5445,  0.5801,\n",
       "         0.0781,  0.6459,  0.2147,  0.7238,  0.6472,  0.5593,  0.0615,  0.5411,\n",
       "         0.5241,  0.5330,  0.4847,  0.5169,  0.4657,  0.5749,  0.5453,  0.4864,\n",
       "         0.6101,  0.4473,  0.5550, -0.0087,  0.5267,  0.4612,  0.4751,  0.5403,\n",
       "         0.5501,  0.4958,  0.5626, -0.0142,  0.5247,  0.4129,  0.5264,  0.0288,\n",
       "         0.5406,  0.5412, -0.0361,  0.5246,  0.0463,  0.5166,  0.5239],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt[1][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, output_mask, batch_encoding = model_pid.forward(ex_in['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4796, -0.1077, -0.6213,  ...,  0.3144, -0.0081,  0.1680],\n",
       "        [ 0.7578,  0.3152, -0.3741,  ...,  0.2359,  0.3185, -0.0199],\n",
       "        [ 0.1988,  0.1710,  0.2189,  ...,  0.0235,  0.2823, -0.1460],\n",
       "        ...,\n",
       "        [ 0.4766, -0.0880, -0.6280,  ...,  0.2449, -0.0965,  0.2291],\n",
       "        [-0.0333, -0.1915, -0.0106,  ..., -0.0877, -0.1617,  0.0835],\n",
       "        [ 0.5764, -0.0884, -0.5672,  ...,  0.2209, -0.1072,  0.1990]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 2, 3, 3, 4, 5, 5, 6, 7, 8, 8, 9, 10, 11, 12, 13, 13, 14, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 33, 34, 34, 35, 36, 37, 38, 39, 40, 41, 41, 42, 42, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "print(batch_encoding.word_ids(batch_index=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([     0,    581,  21455,    509,  36510,    297,    450,      6,      4,\n",
       "            47,  71864,    115, 182867,    136,     47, 135812,  39210,  41361,\n",
       "             7,      6,      4,     10,  27798,  91853,    111, 150385,    509,\n",
       "         56065,  17721,     70,  63557,    111,  79048,    136,   7142, 184593,\n",
       "           136,     70,  18150,  62458,      7,      6,      4,    237,   5299,\n",
       "           237,  54940,     70,  18150,  62458,      7,      6,      5,      2,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_encoding['input_ids'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 43\n"
     ]
    }
   ],
   "source": [
    "print(sum(output_mask[1]), len(ex_in['words'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pid.tokenizer.decode(test_prediction['input_ids'][1][7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [93], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_pid\u001b[39m.\u001b[39;49mforward(ex_in[\u001b[39m'\u001b[39;49m\u001b[39mwords\u001b[39;49m\u001b[39m'\u001b[39;49m])\u001b[39m.\u001b[39;49mshape, \u001b[39mlen\u001b[39m(ex_in[\u001b[39m'\u001b[39m\u001b[39mwords\u001b[39m\u001b[39m'\u001b[39m]), \u001b[39mmax\u001b[39m([\u001b[39mlen\u001b[39m(e) \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m ex_in[\u001b[39m'\u001b[39m\u001b[39mwords\u001b[39m\u001b[39m'\u001b[39m]])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "model_pid.forward(ex_in['words']).shape, len(ex_in['words']), max([len(e) for e in ex_in['words']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: 278,376,625\n",
      "trainable parameters: 278,376,625\n",
      "non-trainable parameters: 0\n"
     ]
    }
   ],
   "source": [
    "print_summary(model_pid, short=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_files.utils.Trainer_pid import Trainer_pid\n",
    "\n",
    "trainer_pid = Trainer_pid()\n",
    "\n",
    "history = trainer_pid.train(\n",
    "    model_pid, optimizer, dataloader_train_en, dataloader_dev_en,\n",
    "    epochs=60, device=device,\n",
    "    save_best=True, \n",
    "    min_score=0.8,\n",
    "    save_path_name=os.path.join(model_dir_path, 'pid_transformer_weights.pth'),\n",
    "    saved_history=history\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nuans_minihw2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a8ff4b25b18867855edb86ba2aaa718c4e3e5e5df1f72ad6de2c0263a3e32427"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
