{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["This notebook generates the SynsetExplorer component and its results that will be useful for the generation of the dataset"]},{"cell_type":"markdown","metadata":{},"source":["# Setup dependencies and variables"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-11-03T20:39:35.092966Z","iopub.status.busy":"2022-11-03T20:39:35.092557Z","iopub.status.idle":"2022-11-03T20:39:49.439754Z","shell.execute_reply":"2022-11-03T20:39:49.438834Z","shell.execute_reply.started":"2022-11-03T20:39:35.092933Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: nltk in /home/marco/miniconda3/envs/nuans_minihw2/lib/python3.9/site-packages (3.7)\n","Requirement already satisfied: regex>=2021.8.3 in /home/marco/miniconda3/envs/nuans_minihw2/lib/python3.9/site-packages (from nltk) (2022.10.31)\n","Requirement already satisfied: tqdm in /home/marco/miniconda3/envs/nuans_minihw2/lib/python3.9/site-packages (from nltk) (4.64.1)\n","Requirement already satisfied: joblib in /home/marco/miniconda3/envs/nuans_minihw2/lib/python3.9/site-packages (from nltk) (1.2.0)\n","Requirement already satisfied: click in /home/marco/miniconda3/envs/nuans_minihw2/lib/python3.9/site-packages (from nltk) (8.1.3)\n"]}],"source":["! pip install nltk"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-11-03T20:39:49.441935Z","iopub.status.busy":"2022-11-03T20:39:49.441643Z","iopub.status.idle":"2022-11-03T20:39:59.295892Z","shell.execute_reply":"2022-11-03T20:39:59.294896Z","shell.execute_reply.started":"2022-11-03T20:39:49.441908Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/marco/miniconda3/envs/nuans_minihw2/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","[nltk_data] Downloading package wordnet to /home/marco/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /home/marco/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import os, sys\n","\n","import numpy as np\n","import torch\n","\n","import requests\n","import zipfile\n","\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import wordnet\n","from collections import Counter\n","from copy import deepcopy\n","\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["True"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["wordnet.synset('entity.n.01').hyponyms()[0] in wordnet.synset('entity.n.01').hyponyms()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-11-03T20:39:59.298306Z","iopub.status.busy":"2022-11-03T20:39:59.297076Z","iopub.status.idle":"2022-11-03T20:39:59.303147Z","shell.execute_reply":"2022-11-03T20:39:59.302385Z","shell.execute_reply.started":"2022-11-03T20:39:59.298252Z"},"trusted":true},"outputs":[],"source":["va_download_url = 'https://verbatlas.org/downloads/VerbAtlas-1.1.0.zip'\n","\n","va_res_path = './va_resources/'\n","os.makedirs(va_res_path, exist_ok=True)\n","va_folder_path = os.path.join(va_res_path,'VerbAtlas-1.1.0')\n","va_root_tsvs = os.path.join(va_folder_path, 'VerbAtlas-1.1.0')\n","\n","va_results_name = 'results.tsv'\n","va_results_path = os.path.join(va_res_path, va_results_name)\n","\n","datasets_path = './datasets/'"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-11-03T20:39:59.307843Z","iopub.status.busy":"2022-11-03T20:39:59.305998Z","iopub.status.idle":"2022-11-03T20:39:59.323423Z","shell.execute_reply":"2022-11-03T20:39:59.322413Z","shell.execute_reply.started":"2022-11-03T20:39:59.307810Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["data already downloaded.\n"]}],"source":["if not os.path.isdir(va_folder_path):\n","    print('downloading data...')\n","    \n","    r = requests.get(va_download_url, stream=True)\n","    with open(va_folder_path+'.zip', 'wb') as fd:\n","        for chunk in r.iter_content(chunk_size=128):\n","            fd.write(chunk)\n","    with zipfile.ZipFile(va_folder_path+'.zip',\"r\") as zip_ref:\n","        zip_ref.extractall(va_res_path)\n","\n","    print('data download complete.')\n","    os.remove(va_folder_path+'.zip')\n","\n","else:\n","    print('data already downloaded.')"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-11-03T20:39:59.325227Z","iopub.status.busy":"2022-11-03T20:39:59.324827Z","iopub.status.idle":"2022-11-03T20:39:59.335706Z","shell.execute_reply":"2022-11-03T20:39:59.334778Z","shell.execute_reply.started":"2022-11-03T20:39:59.325154Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'cuda'"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","device"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.048851,"end_time":"2022-03-31T22:14:13.015246","exception":false,"start_time":"2022-03-31T22:14:12.966395","status":"completed"},"tags":[]},"source":["Setting the seed for reproducibility:"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-11-03T20:39:59.337042Z","iopub.status.busy":"2022-11-03T20:39:59.336764Z","iopub.status.idle":"2022-11-03T20:39:59.347643Z","shell.execute_reply":"2022-11-03T20:39:59.346887Z","shell.execute_reply.started":"2022-11-03T20:39:59.337016Z"},"trusted":true},"outputs":[],"source":["SEED = 28\n","\n","# random.seed(SEED) # not used\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True"]},{"cell_type":"markdown","metadata":{},"source":["# Creating the class for exploring WordNet and connecting it with BabelNet and VerbAtlas"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-11-03T21:16:06.099007Z","iopub.status.busy":"2022-11-03T21:16:06.098691Z","iopub.status.idle":"2022-11-03T21:16:06.150246Z","shell.execute_reply":"2022-11-03T21:16:06.149246Z","shell.execute_reply.started":"2022-11-03T21:16:06.098982Z"},"trusted":true},"outputs":[],"source":["class SynsetExplorer():\n","    def __init__(self, tsvs_folder, wordnet):\n","        \"\"\"A class useful for mapping and exploring VerbAtlas, BabelNet and WordNet.\n","\n","        Args:\n","            tsvs_folder (string): the path to the folder containing all .tsv files necessary for mapping synsets (e.g. VA_bn2va.tsv)\n","            wordnet (WordNetCorpusReader): the instance of the WordNet reader from NLTK\n","        \"\"\"\n","        self.va2bns = {} # from VerbAtlas frame to list of BabelNet synsets\n","        self.bn2va = {} # from BabelNet synset to VerbAtlas synset\n","        self.bn2wn = {} # from BabelNet synset to WordNet synset\n","        self.wn2bn = {} # from WordNet synset to BabelNet synset\n","        self.va2name = {} # from VerbAtlas frame id to VerbAtlas name\n","        self.name2va = {} # from VerbAtlas name to VerbAtlas frame id\n","        self.wn = wordnet\n","        with open(os.path.join(tsvs_folder,'VA_bn2va.tsv')) as file:\n","            for line in file:\n","                l = line.strip('\\n').split('\\t')\n","                if len(l) == 2: # l[0] = bn, l[1] = va\n","                    self.bn2va[l[0]] = l[1]\n","                    self.va2bns[l[1]] = [l[0]] if l[1] not in self.va2bns else self.va2bns[l[1]] + [l[0]]\n","        with open(os.path.join(tsvs_folder,'bn2wn.tsv')) as file:\n","            for line in file:\n","                l = line.strip('\\n').split('\\t')\n","                if len(l) == 2: # l[0] = bn, l[1] = wn\n","                    self.bn2wn[l[0]] = l[1]\n","                    self.wn2bn[l[1]] = l[0]\n","        with open(os.path.join(tsvs_folder,'VA_frame_info.tsv')) as file:\n","            for line in file:\n","                l = line.strip('\\n').split('\\t')\n","                if len(l) > 2: # l[0] = va, l[1] = name\n","                    self.va2name[l[0]] = l[1].upper()\n","                    self.name2va[l[1].upper()] = l[0]\n","                    \n","    def explore_hypernyms(self, negative_synsets = []):\n","        \"\"\"\n","        Explore the graph of the derivationally related forms and their hypernyms, \n","        starting from VerbAtlas frames.\n","\n","        Args:\n","            negative_synsets (list, optional): a list of synsets that, if encountered via DFS on the observed synset, it means\n","            that the observed synset is must not be returned. Defaults to [].\n","\n","        Returns:\n","            list: a list of lists, with this format: [[synset_name, number_of_visits, synset_definition], ...].\\n\n","            The list is ordered by descending oder, so that the first elements are the most visited synsets.\n","        \"\"\"\n","        counter = Counter()\n","        \n","        for va_frame in self.va2bns.keys():\n","            va_frame_name = self.va2name[va_frame]\n","            \n","            for bn_offset_pos in self.va2bns[va_frame]:\n","                wn_offset_pos = self.bn2wn[bn_offset_pos]\n","                wn_synset = self.get_synset_wn(wn_offset_pos)\n","                # for each lemma in the synset, we check its derivationally related form\n","                for wn_lemma in wn_synset.lemmas(): \n","                    wn_der_rel_forms = wn_lemma.derivationally_related_forms()\n","                    # for each derivationally related forms, we collect all hypernyms and\n","                    # their definition, with also how many synsets they are connected to\n","                    for wn_der_rel_form in wn_der_rel_forms: \n","                        wn_der_rel_form_synset = wn_der_rel_form.synset()\n","\n","                        if len(negative_synsets) == 0 or (not self.dfs(wn_der_rel_form_synset, negative_synsets)):\n","\n","                            counter[wn_der_rel_form_synset] += 1\n","                            explore_list = wn_der_rel_form_synset.hypernyms()\n","                            while len(explore_list) > 0:\n","                                popped_synset = explore_list.pop(0)\n","\n","                                if len(negative_synsets) == 0 or (not self.dfs(popped_synset, negative_synsets)):\n","\n","                                    counter[popped_synset] += 1\n","                                explore_list = popped_synset.hypernyms() + explore_list\n","\n","        return [[s.name(), str(n), s.definition()] for s,n in counter.most_common()]\n","\n","    def get_candidates(self, positive_synsets = [], extend_to_hypernyms = False, remove_multiframe_synsets = False):\n","        \"\"\"\n","        Explore the graph of the derivationally related forms and their hypernyms, \n","        starting from VerbAtlas frames.\n","\n","        Args:\n","            positive_synsets (list, optional): a list of synsets that, if encountered via DFS on the observed synset, it means\n","            that the observed synset is a possible candidate for the events. Defaults to [].\n","            extend_to_hypernyms (bool, optional): if true, the synsets saved in the results are also hypernyms of derivationally related forms. Defaults to False.\n","            remove_multiframe_synsets (bool, optional): if true, the synsets that have more than one associated frame are excluded. Defaults to False.\n","\n","        Returns:\n","            dict: a dictionary, where the key is the synset name and the values are:\\n\n","            - definition (str): the string definition of that synset\\n\n","            - frames (set): the set of frames where its verbal synsets are directly connected with it (using derivationally related form)\\n\n","            - indirect_frames (set): the set of frames where its verbal synsets are connected with it using hypernyms\\n\n","            - wn_bases (set): the set of verbal synsets that are directly connected with it (using derivationally related form)\\n\n","            - indirect_wn_bases (set): the set of verbal synsets that are connected with it using hypernyms\\n\n","            - is_drf: if that synset is directly connected with a frame (using derivationally related form)\n","        \"\"\"\n","        result = {}\n","        for va_frame in self.va2bns.keys():\n","            va_frame_name = self.va2name[va_frame]\n","            \n","            for bn_offset_pos in self.va2bns[va_frame]:\n","                wn_offset_pos = self.bn2wn[bn_offset_pos]\n","                wn_synset = self.get_synset_wn(wn_offset_pos)\n","                # for each lemma in the synset, we check its derivationally related forms candidates\n","                for wn_lemma in wn_synset.lemmas(): \n","                    wn_der_rel_forms = wn_lemma.derivationally_related_forms()\n","                    # for each derivationally related forms candidates, we check if by exploring\n","                    # the hypernym path we find any synset included in positive_synsets. If so,\n","                    # that candidate is included in the result.\n","                    for wn_der_rel_form in wn_der_rel_forms: \n","                        wn_der_rel_form_synset = wn_der_rel_form.synset()\n","                        if self.dfs(wn_der_rel_form_synset, positive_synsets):\n","                            if wn_der_rel_form_synset.name() not in result:\n","                                result[wn_der_rel_form_synset.name()] = {\n","                                    'definition':wn_der_rel_form_synset.definition(), \n","                                    'frames':{va_frame_name}, 'indirect_frames':set(), \n","                                    'wn_bases':{wn_synset.name()}, 'indirect_wn_bases':set(), 'is_drf': True}\n","                            else:\n","                                result[wn_der_rel_form_synset.name()]['frames'].add(va_frame_name)\n","                                result[wn_der_rel_form_synset.name()]['wn_bases'].add(wn_synset.name())\n","                            result[wn_der_rel_form_synset.name()]['is_drf'] = True\n","\n","                            if extend_to_hypernyms:\n","                                explore_list = wn_der_rel_form_synset.hypernyms()\n","                                while len(explore_list) > 0:\n","                                    popped_synset = explore_list.pop(0)\n","\n","                                    if self.dfs(popped_synset, positive_synsets):\n","                                        if popped_synset.name() not in result:\n","                                            result[popped_synset.name()] = {\n","                                                'definition':popped_synset.definition(), \n","                                                'frames':set(), 'indirect_frames':{va_frame_name}, \n","                                                'wn_bases':set(), 'indirect_wn_bases':{wn_synset.name()}, 'is_drf': False}\n","                                        else:\n","                                            result[popped_synset.name()]['indirect_frames'].add(va_frame_name)\n","                                            result[popped_synset.name()]['indirect_wn_bases'].add(wn_synset.name())\n","\n","                                    explore_list = popped_synset.hypernyms() + explore_list\n","\n","        return {k:v for k,v in result.items() if len(v['frames']) == 1} if remove_multiframe_synsets else result\n","\n","    def resolve_ambiguity_statistically(self, positive_synsets = [], print_resolution = False):\n","        \"\"\"\n","        It uses the returning dictionary in get_candidates() (see documentation).\\n\n","        The frame in which the derivationally related form is connected the most (or its hypernyms if there is uncertainty), it will be put in it.\n","\n","        Args:\n","            positive_synsets (list, optional): a list of synsets that, if encountered via DFS on the observed synset, it means\n","            that the observed synset is a possible candidate for the events. Defaults to [].\n","            print_resolution (bool, optional): if true, it prints out the resolved frame for each synset. Use for debugging. Defaults to False.\n","\n","        Returns:\n","            dict: the dictionary returned from get_candidates(), but without hypernyms and solving ambiguity.\n","        \"\"\"\n","        candidates_extended = self.get_candidates(positive_synsets, extend_to_hypernyms=True, remove_multiframe_synsets=False)\n","        for candidate_synset_name, candidate_values in candidates_extended.items():\n","            \n","            if len(candidate_values['frames']) > 1 and candidate_values['is_drf']: # ambiguous candidate connected to a baseframe\n","                frames_connections = {\n","                    frame_name:{\n","                        'current' : set(), # current verbal synsets of frame_name encountered by the candidate synset (or its hypernyms)\n","                        'v_synsets' : len(set( self.get_synset_wn(self.bn2wn[bn]).name() for bn in self.va2bns[self.name2va[frame_name]] )) # number of verbal synsets of frame_name\n","                    } for frame_name in candidate_values['frames']\n","                }\n","                # implementing a DFS using a list: if there is only one frame in which the candidate synset is directly connected the most (through derivationally related form),\n","                # then the candidate synset will be put in that frame; else if multiple frames have the same number of direct connections, the ambiguity is solved by using also\n","                # the hypernyms until there will be only one predominant frame\n","                curr_candidate_synset_list = [ self.wn.synset(candidate_synset_name) ]\n","                candidate_frame = list(candidate_values['frames'])[0] # random taking here, it will be updated after with the right candidate\n","                while True:\n","                    curr_candidate_synset = curr_candidate_synset_list.pop(0)\n","                    curr_candidate_synset_name = curr_candidate_synset.name()\n","                    if curr_candidate_synset_name in candidates_extended.keys():\n","                        for wn_v_direct in candidates_extended[curr_candidate_synset_name]['wn_bases']:\n","                            va_frame_origin_name = self.va2name[self.bn2va[self.wn2bn[self.get_encoded_wn(self.wn.synset(wn_v_direct))]]]\n","                            if va_frame_origin_name in frames_connections.keys():\n","                                frames_connections[va_frame_origin_name]['current'].add(wn_v_direct)\n","                    \n","                    best_frames = {k:v for k,v in frames_connections.items() if len(v['current']) == len(frames_connections[max(frames_connections, key = lambda f: len(frames_connections[f]['current']))]['current']) }\n","                    candidate_frame = list(best_frames.keys())[0]\n","                    curr_candidate_synset_list = curr_candidate_synset.hypernyms() + curr_candidate_synset_list\n","                    if len(best_frames) == 1 or curr_candidate_synset_list == []:\n","                        break\n","                \n","                if print_resolution:\n","                    print(candidate_synset_name,'with frames',candidates_extended[candidate_synset_name]['frames'],'is put in',candidate_frame)\n","\n","                candidates_extended[candidate_synset_name]['frames'] = {candidate_frame} # ambiguity resolved\n","\n","        return {k:v for k,v in candidates_extended.items() if v['is_drf']} # removing hypernyms from results\n","\n","    def dfs(self, wn_synset, positive_synsets = []):\n","        \"\"\"Simple implementation of a Depth First Search algorithm\n","\n","        Args:\n","            wn_synset (Synset): a WordNet Synset from wich to start from\n","            positive_synsets (list, optional): the names of the synsets that, if encountered, stops the DFS search \n","            and returns true. If none of them is ever encounteted, then the algorithm returns false. Defaults to [].\n","\n","        Returns:\n","            bool: If true, then the starting synset or one of its hypernyms are present in the positive_synsets. If false,\n","            then the algorithm didn't find any of the synsets name in the positive_synsets list.\n","        \"\"\"\n","        if wn_synset.name() in positive_synsets:\n","            return True\n","        for wn_synset_hypernym in wn_synset.hypernyms():\n","            if self.dfs(wn_synset_hypernym, positive_synsets):\n","                return True\n","        return False\n","\n","    def calculate_frames_connections(self, candidates = {}):\n","        \"\"\"Compute how many VerbAtlas frames are connected with nominal synsets via a counter.\n","\n","        Args:\n","            candidates (dict, optional): all possible candidates for the specified problem, generally\n","            computed by get_candidates(). Defaults to {}.\n","\n","        Returns:\n","            list: a list of pairs in that format: [(frame_name, number_of_connections), ...]\n","        \"\"\"\n","        frames_connections = Counter({n:0 for k,n in self.va2name.items()})\n","        for cf in candidates.values():\n","            for frame in cf['frames']:\n","                frames_connections[frame] += 1\n","        return frames_connections.most_common()\n","\n","    def _path_finding(self, syn_name_from,syn_name_to, limit = 80, path = []):\n","        \"\"\"function used for path_finding function. Do not use separately.\n","\n","        Args:\n","            s_name_from (str): the WordNet verbal synset that is in a VerbAtlas frame to start with.\n","            s_name_to (str): the WordNet nominal synset to find.\n","            limit (int, optional): limit of recursion. The higher, the better for finding paths, but more expensive. Defaults to 80.\n","            path (list, optional): list used recursively to save final path. Do not initialize it. Defaults to [].\n","\n","        Returns:\n","            list: list of synsets that represent the path chosen\n","        \"\"\"\n","        if syn_name_from == syn_name_to:\n","            return path + [syn_name_from]\n","        elif limit == 0:\n","            return []\n","        for wn_synset_hypernym in self.wn.synset(syn_name_from).hypernyms():\n","            new_path = self._path_finding(wn_synset_hypernym.name(), syn_name_to, limit - 1, path+[syn_name_from])\n","            if new_path != []:\n","                return new_path\n","        return path + [syn_name_from]\n","\n","    def path_finding(self, s_name_from, s_name_to, positive_synsets = []):\n","        \"\"\"Find the path between a synset in a VerbAtlas frame and a possible nominal synset.\n","\n","        Args:\n","            s_name_from (str): the WordNet verbal synset that is in a VerbAtlas frame to start with.\n","            s_name_to (str): the WordNet nominal synset to find.\n","            positive_synsets (list, optional): a list of synsets that, if encountered via DFS on the observed synset, it means\n","            that the observed synset is a possible candidate for the events. Defaults to [].\n","\n","        Returns:\n","            str: A detailed string representing the path computed to go from s_name_from to s_name_to\n","        \"\"\"\n","        explored_graph_list = list(self.get_candidates(positive_synsets, extend_to_hypernyms = True, remove_multiframe_synsets = False).keys())\n","        s = self.wn.synset(s_name_from)\n","        va_frame_origin = self.va2name[self.bn2va[self.wn2bn[self.get_encoded_wn(s)]]]\n","        \n","        for l in s.lemmas():\n","            for drf in l.derivationally_related_forms():\n","                ss = drf.synset()\n","                pp = self._path_finding(ss.name(), s_name_to)\n","                if all(elem in explored_graph_list for elem in pp):\n","                    return f'|VA_FRAME:{va_frame_origin}| {s.name()} --lemma--> {l.name()} --drf--> {drf.name()} --synset--> {pp[0]} {\"--hypern-->\" if len(pp[1:])>0 else \"\"} {\" --hypern--> \".join(pp[1:])}'\n","    \n","    def save_results(self, list_of_lists, output_path):\n","        \"\"\"Saves a list of lists in a .tsv format.\n","\n","        Args:\n","            list_of_lists (list): a list of lists\n","            output_path (str): the output filepath\n","        \"\"\"\n","        with open(os.path.join(output_path), 'w') as tsvfile:\n","            for values in list_of_lists:\n","                row = '\\t'.join(values)\n","                print(row, file=tsvfile)\n","\n","    def get_synset_wn(self, wn_offset_pos):\n","        \"\"\"Returns the NLTK Synset instance from a offset-POS string synset\n","\n","        Args:\n","            wn_offset_pos (str): a string of that format: \"wn:<offset><POS>\"\n","\n","        Returns:\n","            Synset: the NLTK Synset instance\n","        \"\"\"\n","        return self.wn.synset_from_pos_and_offset( wn_offset_pos[-1] , int(wn_offset_pos[3:-1]) )\n","\n","    def get_encoded_wn(self, wn_synset):\n","        \"\"\"Returns the offset-POS string synset from a NLTK Synset instance\n","\n","        Args:\n","            wn_synset (Synset): the NLTK Synset instance\n","\n","        Returns:\n","            str: a string of that format: \"wn:<offset><POS>\"\n","        \"\"\"\n","        wn_pos = str(wn_synset.pos())\n","        wn_offset = str(wn_synset.offset())\n","        wn_offset = (8-len(wn_offset))*'0' + wn_offset\n","        return 'wn:'+wn_offset+wn_pos\n","\n","\n","\n","explorer = SynsetExplorer(va_root_tsvs, wordnet)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["VerbAtlas frames:"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['ABSORB', 'ABSTAIN_AVOID_REFRAIN', 'ACCOMPANY', 'ACCUSE', 'ACHIEVE', 'ADD', 'ADJUST_CORRECT', 'AFFECT', 'AFFIRM', 'AGREE_ACCEPT', 'AIR', 'ALLY_ASSOCIATE_MARRY', 'ALTERNATE', 'AMASS', 'AMELIORATE', 'ANALYZE', 'ANSWER', 'APPEAR', 'APPLY', 'APPROVE_PRAISE', 'ARGUE-IN-DEFENSE', 'AROUSE_WAKE_ENLIVEN', 'ARRIVE', 'ASCRIBE', 'ASK_REQUEST', 'ASSIGN-SMT-TO-SMN', 'ATTACH', 'ATTACK_BOMB', 'ATTEND', 'ATTRACT_SUCK', 'AUTHORIZE_ADMIT', 'AUTOMATIZE', 'AUXILIARY', 'AUX_MOD', 'BE-LOCATED_BASE', 'BEFRIEND', 'BEGIN', 'BEHAVE', 'BELIEVE', 'BEND', 'BENEFIT_EXPLOIT', 'BETRAY', 'BEWITCH', 'BID', 'BLIND', 'BORDER', 'BREAK_DETERIORATE', 'BREATH_BLOW', 'BRING', 'BULGE-OUT', 'BURDEN_BEAR', 'BURN', 'BURY_PLANT', 'BUY', 'CAGE_IMPRISON', 'CALCULATE_ESTIMATE', 'CANCEL_ELIMINATE', 'CARRY-OUT-ACTION', 'CARRY_TRANSPORT', 'CASTRATE', 'CATCH', 'CATCH_EMBARK', 'CAUSE-MENTAL-STATE', 'CAUSE-SMT', 'CAVE_CARVE', 'CELEBRATE_PARTY', 'CHANGE-APPEARANCE/STATE', 'CHANGE-HANDS', 'CHANGE-TASTE', 'CHANGE_SWITCH', 'CHARGE', 'CHASE', 'CHOOSE', 'CIRCULATE_SPREAD_DISTRIBUTE', 'CITE', 'CLOSE', 'CLOUD_SHADOW_HIDE', 'CO-OPT', 'COLOR', 'COMBINE_MIX_UNITE', 'COME-AFTER_FOLLOW-IN-TIME', 'COME-FROM', 'COMMUNE', 'COMMUNICATE_CONTACT', 'COMMUNIZE', 'COMPARE', 'COMPENSATE', 'COMPETE', 'COMPLEXIFY', 'CONQUER', 'CONSIDER', 'CONSUME_SPEND', 'CONTAIN', 'CONTINUE', 'CONTRACT-AN-ILLNESS_INFECT', 'CONVERT', 'COOK', 'COOL', 'COPULA', 'COPY', 'CORRELATE', 'CORRODE_WEAR-AWAY_SCRATCH', 'CORRUPT', 'COST', 'COUNT', 'COURT', 'COVER_SPREAD_SURMOUNT', 'CREATE_MATERIALIZE', 'CRITICIZE', 'CRY', 'CUT', 'DANCE', 'DEBASE_ADULTERATE', 'DECEIVE', 'DECIDE_DETERMINE', 'DECREE_DECLARE', 'DEFEAT', 'DELAY', 'DERIVE', 'DESTROY', 'DEVELOP_AGE', 'DIET', 'DIM', 'DIP_DIVE', 'DIRECT_AIM_MANEUVER', 'DIRTY', 'DISAPPEAR', 'DISBAND_BREAK-UP', 'DISCARD', 'DISCOURSE-FUNCTION', 'DISCUSS', 'DISLIKE', 'DISMISS_FIRE-SMN', 'DISTINGUISH_DIFFER', 'DIVERSIFY', 'DIVIDE', 'DOWNPLAY_HUMILIATE', 'DRESS_WEAR', 'DRINK', 'DRIVE-BACK', 'DROP', 'DRY', 'EARN', 'EAT_BITE', 'EMBELLISH', 'EMCEE', 'EMIT', 'EMPHASIZE', 'EMPTY_UNLOAD', 'ENCLOSE_WRAP', 'ENDANGER', 'ENJOY', 'ENTER', 'ESTABLISH', 'EXCRETE', 'EXEMPT', 'EXHAUST', 'EXIST-WITH-FEATURE', 'EXIST_LIVE', 'EXPLAIN', 'EXPLODE', 'EXTEND', 'EXTRACT', 'FACE_CHALLENGE', 'FACIAL-EXPRESSION', 'FAIL_LOSE', 'FAKE', 'FALL_SLIDE-DOWN', 'FEEL', 'FIGHT', 'FILL', 'FIND', 'FINISH_CONCLUDE_END', 'FIT', 'FLATTEN_SMOOTHEN', 'FLATTER', 'FLOW', 'FLY', 'FOCUS', 'FOLLOW-IN-SPACE', 'FOLLOW_SUPPORT_SPONSOR_FUND', 'FORGET', 'FRUSTRATE_DISAPPOINT', 'FUEL', 'GENERATE', 'GIVE-BIRTH', 'GIVE-UP_ABOLISH_ABANDON', 'GIVE_GIFT', 'GO-FORWARD', 'GROUND_BASE_FOUND', 'GROUP', 'GROW_PLOW', 'GUARANTEE_ENSURE_PROMISE', 'GUESS', 'HANG', 'HAPPEN_OCCUR', 'HARMONIZE', 'HAVE-A-FUNCTION_SERVE', 'HAVE-SEX', 'HEAR_LISTEN', 'HEAT', 'HELP_HEAL_CARE_CURE', 'HIRE', 'HIT', 'HOLE_PIERCE', 'HOST_MEAL_INVITE', 'HUNT', 'HURT_HARM_ACHE', 'IMAGINE', 'IMPLY', 'INCITE_INDUCE', 'INCLINE', 'INCLUDE-AS', 'INCREASE_ENLARGE_MULTIPLY', 'INFER', 'INFLUENCE', 'INFORM', 'INSERT', 'INTERPRET', 'INVERT_REVERSE', 'ISOLATE', 'JOIN_CONNECT', 'JOKE', 'JUMP', 'JUSTIFY_EXCUSE', 'KILL', 'KNOCK-DOWN', 'KNOW', 'LAND_GET-OFF', 'LAUGH', 'LEAD_GOVERN', 'LEARN', 'LEAVE-BEHIND', 'LEAVE_DEPART_RUN-AWAY', 'LEND', 'LIBERATE_ALLOW_AFFORD', 'LIE', 'LIGHT-VERB', 'LIGHTEN', 'LIGHT_SHINE', 'LIKE', 'LOAD_PROVIDE_CHARGE_FURNISH', 'LOCATE-IN-TIME_DATE', 'LOSE', 'LOWER', 'LURE_ENTICE', 'MAKE-A-SOUND', 'MAKE-RELAX', 'MANAGE', 'MATCH', 'MEAN', 'MEASURE_EVALUATE', 'MEET', 'MESS', 'METEOROLOGICAL', 'MISS_OMIT_LACK', 'MISTAKE', 'MODAL', 'MOUNT_ASSEMBLE_PRODUCE', 'MOVE-BACK', 'MOVE-BY-MEANS-OF', 'MOVE-ONESELF', 'MOVE-SOMETHING', 'MUST', 'NAME', 'NEGOTIATE', 'NOURISH_FEED', 'OBEY', 'OBLIGE_FORCE', 'OBTAIN', 'ODORIZE', 'OFFEND_DISESTEEM', 'OFFER', 'OPEN', 'OPERATE', 'OPPOSE_REBEL_DISSENT', 'ORDER', 'ORGANIZE', 'ORIENT', 'OVERCOME_SURPASS', 'OVERLAP', 'PAINT', 'PARDON', 'PARTICIPATE', 'PAY', 'PERCEIVE', 'PERFORM', 'PERMEATE', 'PERSUADE', 'PLAN_SCHEDULE', 'PLAY_SPORT/GAME', 'POPULATE', 'POSSESS', 'PRECEDE', 'PRECLUDE_FORBID_EXPEL', 'PREPARE', 'PRESERVE', 'PRESS_PUSH_FOLD', 'PRETEND', 'PRINT', 'PROMOTE', 'PRONOUNCE', 'PROPOSE', 'PROTECT', 'PROVE', 'PUBLICIZE', 'PUBLISH', 'PULL', 'PUNISH', 'PUT_APPLY_PLACE_PAVE', 'QUARREL_POLEMICIZE', 'RAISE', 'REACH', 'REACT', 'READ', 'RECALL', 'RECEIVE', 'RECOGNIZE_ADMIT_IDENTIFY', 'RECORD', 'REDUCE_DIMINISH', 'REFER', 'REFLECT', 'REFUSE', 'REGRET_SORRY', 'RELY', 'REMAIN', 'REMEMBER', 'REMOVE_TAKE-AWAY_KIDNAP', 'RENEW', 'REPAIR_REMEDY', 'REPEAT', 'REPLACE', 'REPRESENT', 'REPRIMAND', 'REQUIRE_NEED_WANT_HOPE', 'RESERVE', 'RESIGN_RETIRE', 'RESIST', 'REST', 'RESTORE-TO-PREVIOUS/INITIAL-STATE_UNDO_UNWIND', 'RESTRAIN', 'RESULT_CONSEQUENCE', 'RETAIN_KEEP_SAVE-MONEY', 'REVEAL', 'RISK', 'ROLL', 'RUN', 'SATISFY_FULFILL', 'SCORE', 'SEARCH', 'SECURE_FASTEN_TIE', 'SEE', 'SEEM', 'SELL', 'SEND', 'SEPARATE_FILTER_DETACH', 'SETTLE_CONCILIATE', 'SEW', 'SHAPE', 'SHARE', 'SHARPEN', 'SHOOT_LAUNCH_PROPEL', 'SHOUT', 'SHOW', 'SIGN', 'SIGNAL_INDICATE', 'SIMPLIFY', 'SIMULATE', 'SING', 'SLEEP', 'SLOW-DOWN', 'SMELL', 'SOLVE', 'SORT_CLASSIFY_ARRANGE', 'SPEAK', 'SPEED-UP', 'SPEND-TIME_PASS-TIME', 'SPILL_POUR', 'SPOIL', 'STABILIZE_SUPPORT-PHYSICALLY', 'START-FUNCTIONING', 'STAY_DWELL', 'STEAL_DEPRIVE', 'STOP', 'STRAIGHTEN', 'STRENGTHEN_MAKE-RESISTANT', 'STUDY', 'SUBJECTIVE-JUDGING', 'SUBJUGATE', 'SUMMARIZE', 'SUMMON', 'SUPPOSE', 'SWITCH-OFF_TURN-OFF_SHUT-DOWN', 'TAKE', 'TAKE-A-SERVICE_RENT', 'TAKE-INTO-ACCOUNT_CONSIDER', 'TAKE-SHELTER', 'TASTE', 'TEACH', 'THINK', 'THROW', 'TIGHTEN', 'TOLERATE', 'TOUCH', 'TRANSLATE', 'TRANSMIT', 'TRAVEL', 'TREAT', 'TREAT-WITH/BY', 'TRY', 'TURN_CHANGE-DIRECTION', 'TYPE', 'UNDERGO-EXPERIENCE', 'UNDERSTAND', 'UNFASTEN_UNFOLD', 'USE', 'VERIFY', 'VIOLATE', 'VISIT', 'WAIT', 'WARN', 'WASH_CLEAN', 'WASTE', 'WATCH_LOOK-OUT', 'WEAKEN', 'WEAVE', 'WELCOME', 'WET', 'WIN', 'WORK', 'WORSEN', 'WRITE']\n"]}],"source":["(print(sorted(list(explorer.name2va.keys()))))"]},{"cell_type":"markdown","metadata":{},"source":["Each VerbAtlas Frame is composed of a cluster of BabelNet synsets:"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-11-03T20:39:59.398598Z","iopub.status.busy":"2022-11-03T20:39:59.397934Z","iopub.status.idle":"2022-11-03T20:39:59.403442Z","shell.execute_reply":"2022-11-03T20:39:59.402405Z","shell.execute_reply.started":"2022-11-03T20:39:59.398571Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['bn:00083099v', 'bn:00083503v', 'bn:00083515v', 'bn:00083519v', 'bn:00083752v', 'bn:00083753v', 'bn:00083755v', 'bn:00084009v', 'bn:00084145v', 'bn:00084147v', 'bn:00084153v', 'bn:00084464v', 'bn:00084764v', 'bn:00084765v', 'bn:00084843v', 'bn:00084888v', 'bn:00085688v', 'bn:00085689v', 'bn:00086112v', 'bn:00086765v', 'bn:00086815v', 'bn:00086819v', 'bn:00086843v', 'bn:00086845v', 'bn:00087374v', 'bn:00087460v', 'bn:00087461v', 'bn:00087462v', 'bn:00087468v', 'bn:00088039v', 'bn:00088181v', 'bn:00088449v', 'bn:00088574v', 'bn:00088759v', 'bn:00088911v', 'bn:00088950v', 'bn:00089118v', 'bn:00090519v', 'bn:00090694v', 'bn:00090748v', 'bn:00091055v', 'bn:00091056v', 'bn:00091057v', 'bn:00091065v', 'bn:00091091v', 'bn:00091449v', 'bn:00091507v', 'bn:00091509v', 'bn:00091611v', 'bn:00091847v', 'bn:00091942v', 'bn:00092210v', 'bn:00092393v', 'bn:00093167v', 'bn:00093334v', 'bn:00093918v', 'bn:00093968v', 'bn:00094593v', 'bn:00094750v', 'bn:00095538v', 'bn:00095654v', 'bn:00095787v', 'bn:00095810v', 'bn:00095830v']\n"]}],"source":["va_frame_example = 'va:0051f' # = EAT_BITE\n","bn_synsets_offset_example = explorer.va2bns[va_frame_example]\n","print(bn_synsets_offset_example)"]},{"cell_type":"markdown","metadata":{},"source":["Each BabelNet synset can be converted into a WordNet synset:"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["e.g.: bn:00083099v -> wn:01185981v = Synset('feast.v.01')\n","['feast.v.01', 'gorge.v.01', 'bite.v.01', 'snap_at.v.01', 'bolt.v.03', 'gobble.v.01', 'garbage_down.v.01', 'breakfast.v.01', 'crop.v.05', 'browse.v.04', 'brunch.v.01', 'cannibalize.v.01', 'champ.v.01', 'chomp.v.01', 'chaw.v.01', 'chew.v.01', 'devour.v.03', 'consume.v.02', 'crunch.v.03', 'devour.v.04', 'pitch_in.v.01', 'digest.v.01', 'dine.v.01', 'eat_in.v.01', 'drop.v.17', 'eat.v.01', 'eat.v.02', 'feed.v.06', 'eat_up.v.01', 'fare.v.02', 'fill_up.v.04', 'forage.v.02', 'gluttonize.v.01', 'swallow.v.01', 'gnaw.v.01', 'go_down.v.05', 'mumble.v.02', 'lunch.v.01', 'masticate.v.01', 'mess.v.01', 'nibble.v.01', 'nibble.v.02', 'nibble.v.03', 'nip.v.02', 'nosh.v.01', 'partake.v.03', 'peck.v.02', 'pick_at.v.02', 'picnic.v.01', 'pop.v.11', 'predigest.v.01', 'tuck_in.v.01', 'raven.v.04', 'ruminate.v.01', 'scavenge.v.03', 'slurp.v.01', 'snap.v.12', 'sup.v.01', 'take_out.v.12', 'victual.v.03', 'wash_down.v.01', 'wine_and_dine.v.01', 'wolf.v.01', 'worry.v.05']\n"]}],"source":["wn_synset_offset_example = explorer.bn2wn[bn_synsets_offset_example[0]]\n","print('e.g.:',bn_synsets_offset_example[0], '->', wn_synset_offset_example, '=', explorer.get_synset_wn(wn_synset_offset_example))\n","print([explorer.get_synset_wn(explorer.bn2wn[e]).name() for e in bn_synsets_offset_example])"]},{"cell_type":"markdown","metadata":{},"source":["The first thing to notice is that a very small part of the VerbAtlas frames have no BabelNet synsets connected to them:"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["425 VerbAtlas frames are mapped into BabelNet synsets out of 432\n","The total number of BabelNet synsets in the latter is 13767\n"]}],"source":["n_of_bn_synsets = 0\n","for va_frame in explorer.va2bns.keys():\n","    for bn_offset_pos in explorer.va2bns[va_frame]:\n","        n_of_bn_synsets+=1\n","\n","print(len(explorer.va2bns.keys()),'VerbAtlas frames are mapped into BabelNet synsets out of',len(explorer.va2name.keys()))\n","print('The total number of BabelNet synsets in the latter is',n_of_bn_synsets)"]},{"cell_type":"markdown","metadata":{},"source":["# Exploring WordNet graph"]},{"cell_type":"markdown","metadata":{},"source":["In order to identify (and exploit) the most common synsets in the graph, some procedures must be done: <br>\n","<ul>\n","    <li> For each VerbAtlas frame, we pick every BabelNet synset.\n","    <li> For each of them, we convert it to the corresponding WordNet synset.\n","    <li> Then, by using the derivationally related forms of the lemmas of each WordNet synset, we explore the corresponding hypernyms recursively, via DFS. \n","</ul>"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-11-03T21:16:06.452838Z","iopub.status.busy":"2022-11-03T21:16:06.452504Z","iopub.status.idle":"2022-11-03T21:16:07.119213Z","shell.execute_reply":"2022-11-03T21:16:07.117994Z","shell.execute_reply.started":"2022-11-03T21:16:06.452810Z"},"trusted":true},"outputs":[],"source":["explored_graph = explorer.explore_hypernyms()\n","explorer.save_results(explored_graph, os.path.join(va_res_path,'./explored_graph.tsv'))"]},{"cell_type":"markdown","metadata":{},"source":["The output of the file has three columns:\n","<ul>\n","    <li> the WordNet id\n","    <li> the number of times that the entity is visited\n","    <li> its definition \n","</ul>\n","Here are some examples:"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/plain":["[['entity.n.01',\n","  '27293',\n","  'that which is perceived or known or inferred to have its own distinct existence (living or nonliving)'],\n"," ['abstraction.n.06',\n","  '14845',\n","  'a general concept formed by extracting common features from specific examples'],\n"," ['physical_entity.n.01', '12442', 'an entity that has physical existence'],\n"," ['psychological_feature.n.01',\n","  '9321',\n","  'a feature of the mental life of a living organism']]"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["explored_graph[:4]"]},{"cell_type":"markdown","metadata":{},"source":["# Finding best hypernyms"]},{"cell_type":"markdown","metadata":{},"source":["Now, by searching for possible candidates (using keywords like \"act\", \"event and so on\") and by manually double-checking them using synset's hyponyms and definitions, we can retrieve possible candidates to use in order to identify hyponyms that can be considered nominal events!"]},{"cell_type":"markdown","metadata":{},"source":["Computing the candidates:"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Take the most significant ones and check if there are others not explored by the temporary potential candidates!\n","Let's put the most prominent ones in there and see if (and which) remaining synsets still remains untouched (via DFS):<br>\n","<ol>\n","<li> Explore the graph and prune it with the temporary positive_synsets: i.e., if a synset is an hyponym in the positive_synsets list, it is removed (because the possible candidate synset in the final results.tsv file will have as possible hypernyms the ones present in positive_synsets)\n","<li> Check for remaining synsets in the generated file explored_graph.tsv that could be put in positive_synsets. If so, redo from step 1., else stop\n","</ol>"]},{"cell_type":"markdown","metadata":{},"source":["1)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["positive_synsets = [\n","    'event.n.01',\n","    'act.n.02',\n","    # communication.n.02, # something that is communicated by or to or between people or groups | I don't think it's a good candidate, need to understand better\n","    'process.n.06',\n","    # process.n.02, # (psychology) the performance of some composite cognitive activity; an operation that affects mental contents\n","    # feeling.n.01,\n","    # thinking.n.01,\n","]\n","\n","explored_graph = explorer.explore_hypernyms(negative_synsets=positive_synsets)\n","explorer.save_results(explored_graph, os.path.join(va_res_path,'explored_graph.tsv'))"]},{"cell_type":"markdown","metadata":{},"source":["2)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["# Synset('feeling.n.01') the experiencing of affective and emotional states\n","-----------------------\n","Synset('affect.n.01') the conscious subjective aspect of feeling or emotion\n","Synset('affection.n.01') a positive feeling of liking\n","Synset('agitation.n.03') the feeling of being agitated; not calm\n","Synset('ambivalence.n.01') mixed feelings or emotions\n","Synset('apathy.n.01') an absence of emotion or enthusiasm\n","Synset('astonishment.n.01') the feeling that accompanies something extremely surprising\n","Synset('calmness.n.03') a feeling of calm; an absence of agitation or excitement\n","Synset('complex.n.03') (psychoanalysis) a combination of emotions and impulses that have been rejected from awareness but still influence a person's behavior\n","Synset('desire.n.01') the feeling that accompanies an unsatisfied state\n","Synset('despair.n.02') the feeling that everything is wrong and nothing will turn out well\n","Synset('devastation.n.02') the feeling of being confounded or overwhelmed\n","Synset('dislike.n.02') a feeling of aversion or antipathy\n","Synset('emotion.n.01') any strong feeling\n","Synset('enthusiasm.n.01') a feeling of excitement\n","Synset('expectation.n.03') the feeling that something is about to happen\n","Synset('faintness.n.01') a feeling of faintness and of being ready to swoon\n","Synset('fearlessness.n.01') feeling no fear\n","Synset('glow.n.04') a feeling of considerable warmth\n","Synset('gratitude.n.01') a feeling of thankfulness and appreciation\n","Synset('gravity.n.03') a solemn and dignified feeling\n","Synset('happiness.n.02') emotions experienced when in a state of well-being\n","Synset('hope.n.02') the general feeling that some desire will be fulfilled\n","Synset('humility.n.02') a humble feeling\n","Synset('ingratitude.n.01') a lack of gratitude\n","Synset('levity.n.01') feeling an inappropriate lack of seriousness\n","Synset('liking.n.01') a feeling of pleasure and enjoyment\n","Synset('pain.n.02') emotional distress; a fundamental feeling that people try to avoid\n","Synset('pang.n.01') a sudden sharp feeling\n","Synset('passion.n.01') a strong feeling or emotion\n","Synset('pleasure.n.01') a fundamental feeling that is hard to define but that people desire to experience\n","Synset('pride.n.01') a feeling of self-respect and personal worth\n","Synset('sadness.n.01') emotions experienced when not in a state of well-being\n","Synset('sensitivity.n.03') sensitivity to emotional feelings (of self and others)\n","Synset('sentiment.n.01') tender, romantic, or nostalgic feeling or emotion\n","Synset('sex.n.03') all of the feelings resulting from the urge to gratify sexual impulses\n","Synset('shame.n.01') a painful emotion resulting from an awareness of inadequacy or guilt\n","Synset('soul.n.03') deep feeling or emotion\n","Synset('sympathy.n.02') sharing the feelings of others (especially feelings of sorrow or anguish)\n","Synset('temper.n.02') a characteristic (habitual or relatively temporary) state of feeling\n","Synset('thing.n.11') a persistent illogical feeling of desire or aversion\n","Synset('unconcern.n.02') a feeling of lack of concern\n"]}],"source":["# We can check for both methods synsets that are ambiguous by seeing not only its definition, but also of the hyponyms:\n","excluded_synset = 'feeling.n.01'\n","sysn = wordnet.synset(excluded_synset)\n","print('#', sysn, sysn.definition())\n","print('-----------------------')\n","for ssysn in sysn.hyponyms():\n","    print(ssysn, ssysn.definition())"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["There are 3974 possible candidates!\n"]}],"source":["poss_cand = explorer.get_candidates(positive_synsets, extend_to_hypernyms = False, remove_multiframe_synsets = False)\n","print('There are',len(poss_cand.keys()),'possible candidates!')"]},{"cell_type":"markdown","metadata":{},"source":["We have obtained the same result with less effort and synsets!"]},{"cell_type":"markdown","metadata":{},"source":["With the method get_candidates() we have obtained around 4000 possible candidates to be part of the nominal resource. One possible example:"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["eating.n.01 {'definition': 'the act of consuming food', 'frames': {'EAT_BITE'}, 'indirect_frames': set(), 'wn_bases': {'eat.v.02', 'feed.v.06', 'eat.v.01'}, 'indirect_wn_bases': set(), 'is_drf': True}\n"]}],"source":["ex_pc = 'eating.n.01'\n","print(ex_pc, poss_cand[ex_pc])"]},{"cell_type":"markdown","metadata":{},"source":["# Resolving ambiguity"]},{"cell_type":"markdown","metadata":{},"source":["There is one main problem that was not addressed so far:\n","<ul>\n","    <li> What if a synset is ambiguous? (i.e. it can be in multiple frames?)\n","</ul><br>\n","Here is an example of ambiguity:"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["adoption.n.01 {'definition': 'the act of accepting with approval; favorable reception', 'frames': {'FOLLOW_SUPPORT_SPONSOR_FUND', 'AGREE_ACCEPT', 'TOLERATE'}, 'indirect_frames': set(), 'wn_bases': {'accept.v.04', 'accept.v.03', 'accept.v.02', 'adopt.v.01'}, 'indirect_wn_bases': set(), 'is_drf': True}\n"]}],"source":["ex_amb = 'adoption.n.01'\n","print(ex_amb, poss_cand[ex_amb])"]},{"cell_type":"markdown","metadata":{},"source":["A clever solution must be found. If we remove the ambiguous ones:"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["There are 2899 unambiguous candidates, so we have 1075 ambiguous!\n"]}],"source":["poss_cand_unamb = explorer.get_candidates(positive_synsets, remove_multiframe_synsets = True)\n","print('There are',len(poss_cand_unamb.keys()),'unambiguous candidates, so we have',len(poss_cand.keys())-len(poss_cand_unamb.keys()),'ambiguous!')"]},{"cell_type":"markdown","metadata":{},"source":["There are various solutions that can be made:\n","<ol>\n","<li> Put the ambiguous synset in all the corresponding frames. This could be a solution in order to understand if two or more frames need to be merged.\n","<li> Remove the ambiguous synset in all the corresponding frames. This can't be an optimal solution.\n","<li> Put the ambiguous synset in a particular frame using some heuristics. This is the optimal solution, removing the ambiguity of the frame.\n","</ol>\n","The third solution will be applied, using statistics: the frame in which the derivationally related form is connected the most (or its hypernyms if there is uncertanty), it will be put in it."]},{"cell_type":"markdown","metadata":{},"source":["Here I will print an example of an ambiguous synset candidate, with also its hypernym:"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["articulation.n.03 ---hypern---> expression.n.03\n","--------------- synset: ----------------\n","{'definition': 'expressing in coherent verbal form', 'frames': {'PRONOUNCE', 'SPEAK', 'EXPLAIN'}, 'indirect_frames': set(), 'wn_bases': {'voice.v.01', 'give_voice.v.01', 'pronounce.v.01', 'articulate.v.05'}, 'indirect_wn_bases': set(), 'is_drf': True}\n","--------------- hypernymy: ----------------\n","{'definition': 'the communication (in speech or writing) of your beliefs or opinions', 'frames': {'SPEAK'}, 'indirect_frames': {'CAUSE-MENTAL-STATE', 'SPEAK', 'APPROVE_PRAISE', 'PRONOUNCE', 'EXPLAIN'}, 'wn_bases': {'express.v.02'}, 'indirect_wn_bases': {'voice.v.01', 'congratulate.v.02', 'articulate.v.05', 'pronounce.v.01', 'compliment.v.01', 'pride.v.01', 'give_voice.v.01'}, 'is_drf': True}\n"]}],"source":["candidates_extended = explorer.get_candidates(positive_synsets, extend_to_hypernyms=True, remove_multiframe_synsets=False)\n","\n","candidate_synset_name = 'articulation.n.03'\n","candidate_values = candidates_extended[candidate_synset_name]\n","\n","candidate_synset_name_hyper = wordnet.synset(candidate_synset_name).hypernyms()[0].name()\n","candidate_values_hyper = candidates_extended[candidate_synset_name_hyper]\n","\n","print(candidate_synset_name, '---hypern--->', candidate_synset_name_hyper)\n","print('--------------- synset: ----------------')\n","print(candidate_values)\n","print('--------------- hypernymy: ----------------')\n","print(candidate_values_hyper)"]},{"cell_type":"markdown","metadata":{},"source":["Here, by using path finding for the candidate and its hypernym, I will show the path to reach the candidate starting from the verbal synsets:"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["|VA_FRAME:SPEAK| voice.v.01 --lemma--> voice --drf--> voice --synset--> articulation.n.03  \n","|VA_FRAME:SPEAK| give_voice.v.01 --lemma--> articulate --drf--> articulation --synset--> articulation.n.03  \n","|VA_FRAME:PRONOUNCE| pronounce.v.01 --lemma--> articulate --drf--> articulation --synset--> articulation.n.03  \n","|VA_FRAME:EXPLAIN| articulate.v.05 --lemma--> articulate --drf--> articulation --synset--> articulation.n.03  \n","---------- hypernymy direct connections (as drf) -------\n","|VA_FRAME:SPEAK| express.v.02 --lemma--> express --drf--> expression --synset--> expression.n.03  \n","---------- hypernymy indirect connections (e.g. hypernymy of...) -------\n","|VA_FRAME:SPEAK| voice.v.01 --lemma--> voice --drf--> voice --synset--> articulation.n.03 --hypern--> expression.n.03\n","|VA_FRAME:APPROVE_PRAISE| congratulate.v.02 --lemma--> congratulate --drf--> congratulation --synset--> congratulation.n.02 --hypern--> expression.n.03\n","|VA_FRAME:EXPLAIN| articulate.v.05 --lemma--> articulate --drf--> articulation --synset--> articulation.n.03 --hypern--> expression.n.03\n","|VA_FRAME:PRONOUNCE| pronounce.v.01 --lemma--> articulate --drf--> articulation --synset--> articulation.n.03 --hypern--> expression.n.03\n","|VA_FRAME:APPROVE_PRAISE| compliment.v.01 --lemma--> congratulate --drf--> congratulation --synset--> congratulation.n.02 --hypern--> expression.n.03\n","|VA_FRAME:CAUSE-MENTAL-STATE| pride.v.01 --lemma--> congratulate --drf--> congratulation --synset--> congratulation.n.02 --hypern--> expression.n.03\n","|VA_FRAME:SPEAK| give_voice.v.01 --lemma--> articulate --drf--> articulation --synset--> articulation.n.03 --hypern--> expression.n.03\n"]}],"source":["for wn_base in candidate_values['wn_bases']:\n","    print( explorer.path_finding(wn_base, candidate_synset_name, positive_synsets) )\n","print('---------- hypernymy direct connections (as drf) -------')\n","for wn_base in candidate_values_hyper['wn_bases']:\n","    print( explorer.path_finding(wn_base, candidate_synset_name_hyper, positive_synsets) )\n","print('---------- hypernymy indirect connections (e.g. hypernymy of...) -------')\n","for wn_base in candidate_values_hyper['indirect_wn_bases']:\n","    print( explorer.path_finding(wn_base, candidate_synset_name_hyper, positive_synsets) )"]},{"cell_type":"markdown","metadata":{},"source":["Now, solving ambiguity statistically:"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["There are 0 ambiguous candidates now! Total synsets now:  3974\n"]}],"source":["poss_cand_solved_statistically = explorer.resolve_ambiguity_statistically(positive_synsets=positive_synsets)\n","n_ambiguous_now = {k:v for k,v in poss_cand_solved_statistically.items() if len(v['frames']) > 1}\n","print('There are',len(n_ambiguous_now.keys()),'ambiguous candidates now!', 'Total synsets now: ', len(poss_cand_solved_statistically))"]},{"cell_type":"markdown","metadata":{},"source":["We can also see how many frames are not connected with any noun:"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["There are 37 frames that are not connected with nominal synsets out of 432\n"]}],"source":["frame_connections = explorer.calculate_frames_connections(poss_cand_solved_statistically)\n","frames_not_touched = [e for e in frame_connections if e[1] == 0]\n","print('There are',len(frames_not_touched),'frames that are not connected with nominal synsets out of',len(frame_connections))"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["explorer.save_results([[s,v['definition']] + list(v['frames']) for s,v in poss_cand_solved_statistically.items()], os.path.join(va_res_path,'./possible_candidates.tsv'))"]},{"cell_type":"markdown","metadata":{},"source":["After a final double-check, we are ready to save them!"]},{"cell_type":"markdown","metadata":{},"source":["# Saving final results"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["explorer.save_results([[s,list(v['frames'])[0]] for s,v in poss_cand_solved_statistically.items()], os.path.join(va_res_path,'./results.tsv'))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Saving nominal synset graph"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["import json\n","\n","class SetEncoder(json.JSONEncoder):\n","    def default(self, obj):\n","        if isinstance(obj, set):\n","            return list(obj)\n","        return json.JSONEncoder.default(self, obj)\n","\n","explored_nominal_graph = explorer.get_candidates(positive_synsets, extend_to_hypernyms = True, remove_multiframe_synsets = False)\n","explored_nominal_graph_unambiguous = explorer.get_candidates(positive_synsets, extend_to_hypernyms = True, remove_multiframe_synsets = True)\n","candidates_unambiguous = explorer.get_candidates(positive_synsets, extend_to_hypernyms = False, remove_multiframe_synsets = True)\n","\n","with open(os.path.join(va_res_path,'explored_nominal_graph.json'), 'w') as outfile:\n","    json.dump(explored_nominal_graph, outfile, cls=SetEncoder, indent=4)\n","\n","with open(os.path.join(va_res_path,'explored_nominal_graph_unambiguous.json'), 'w') as outfile:\n","    json.dump(explored_nominal_graph_unambiguous, outfile, cls=SetEncoder, indent=4)\n","\n","with open(os.path.join(va_res_path,'candidates_unambiguous.json'), 'w') as outfile:\n","    json.dump(candidates_unambiguous, outfile, cls=SetEncoder, indent=4)\n","\n","with open(os.path.join(va_res_path,'poss_cand_solved_statistically.json'), 'w') as outfile:\n","    json.dump(poss_cand_solved_statistically, outfile, cls=SetEncoder, indent=4)"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"data":{"text/plain":["(4346, 2899, 2899)"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["len(explored_nominal_graph), len(explored_nominal_graph_unambiguous), len(candidates_unambiguous)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["with open(os.path.join(va_res_path,'positive_synsets.json'), 'w') as outfile:\n","    json.dump(positive_synsets, outfile, cls=SetEncoder, indent=4)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Creating dataset for Nominal Identificator"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["ratio = 0.8\n","\n","ni_train = []\n","ni_valid = []\n","\n","nominal_part = explorer.get_candidates(positive_synsets, extend_to_hypernyms = True, remove_multiframe_synsets = False)\n","ni_train_dim = int(len(nominal_part)*0.8)\n","ni_valid_dim = int(len(nominal_part)*0.2)\n","\n","for i, (wn_syn, vals) in enumerate(nominal_part.items()):\n","    result_sample = [\n","        wn_syn.split('.')[0].replace('_',' '),\n","        vals['definition'], \n","        '1',\n","        wn_syn]\n","    if i < ni_train_dim:\n","        ni_train.append(result_sample)\n","    elif i < ni_train_dim + ni_valid_dim:\n","        ni_valid.append(result_sample)\n","    else:\n","        break\n","\n","num_of_samples = 0\n","for va_frame in explorer.va2bns.keys():\n","    va_frame_name = explorer.va2name[va_frame]\n","    \n","    for bn_offset_pos in explorer.va2bns[va_frame]:\n","        wn_offset_pos = explorer.bn2wn[bn_offset_pos]\n","        wn_synset = explorer.get_synset_wn(wn_offset_pos)\n","        \n","        for wn_lemma in wn_synset.lemmas(): \n","            wn_der_rel_forms = wn_lemma.derivationally_related_forms()\n","            for wn_der_rel_form in wn_der_rel_forms: \n","                wn_der_rel_form_synset = wn_der_rel_form.synset()\n","                if not explorer.dfs(wn_der_rel_form_synset, positive_synsets):\n","\n","                    result_sample = [\n","                        wn_der_rel_form_synset.name().split('.')[0].replace('_',' '), \n","                        wn_der_rel_form_synset.definition(), \n","                        '0',\n","                        wn_der_rel_form_synset.name()]\n","\n","                    if num_of_samples < ni_train_dim:\n","                        ni_train.append(result_sample)\n","                    elif num_of_samples < ni_train_dim + ni_valid_dim:\n","                        ni_valid.append(result_sample)\n","                    else:\n","                        break\n","\n","                    num_of_samples += 1\n","\n","    if num_of_samples >= ni_train_dim + ni_valid_dim:\n","        break\n"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["os.makedirs(os.path.join(datasets_path, 'nominal_iden_dataset'),exist_ok=True)\n","explorer.save_results(ni_train, os.path.join(datasets_path, 'nominal_iden_dataset', 'train.tsv'))\n","explorer.save_results(ni_valid, os.path.join(datasets_path, 'nominal_iden_dataset', 'valid.tsv'))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"nuans_minihw2","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15"},"vscode":{"interpreter":{"hash":"a8ff4b25b18867855edb86ba2aaa718c4e3e5e5df1f72ad6de2c0263a3e32427"}}},"nbformat":4,"nbformat_minor":4}
